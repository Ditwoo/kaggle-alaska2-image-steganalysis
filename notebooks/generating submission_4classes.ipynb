{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "\n",
    "import albumentations as alb\n",
    "import albumentations.pytorch\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmdr/anaconda3/envs/torch/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/home/dmdr/anaconda3/envs/torch/lib/python3.7/site-packages/nbformat/notebooknode.py:4: DeprecationWarning:\n",
      "\n",
      "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "\n",
      "/home/dmdr/anaconda3/envs/torch/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/home/dmdr/anaconda3/envs/torch/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/home/dmdr/anaconda3/envs/torch/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import MulticlassEfficientNet\n",
    "from src.datasets import ImagesDataset\n",
    "\n",
    "normalizations = alb.Compose([alb.Normalize(), alb.pytorch.ToTensorV2()])\n",
    "# normalizations = alb.Compose([alb.ToFloat(max_value=255), alb.pytorch.ToTensorV2()])\n",
    "# normalizations = alb.Compose([alb.Normalize(max_pixel_value=1), alb.pytorch.ToTensorV2()])  # IT IS WRONG\n",
    "\n",
    "ttas = [\n",
    "    # original\n",
    "    alb.Compose([\n",
    "        alb.Resize(512, 512),\n",
    "        normalizations,\n",
    "    ]),\n",
    "    # horizontal flipped\n",
    "    alb.Compose([\n",
    "        alb.Resize(512, 512),\n",
    "        alb.HorizontalFlip(p=1),\n",
    "        normalizations,\n",
    "    ]),\n",
    "    # vertical flipped\n",
    "    alb.Compose([\n",
    "        alb.Resize(512, 512),\n",
    "        alb.VerticalFlip(p=1),\n",
    "        normalizations,\n",
    "    ]),\n",
    "    # horizontal & vertical flipped\n",
    "    alb.Compose([\n",
    "        alb.Resize(512, 512),\n",
    "        alb.HorizontalFlip(p=1),\n",
    "        alb.VerticalFlip(p=1),\n",
    "        normalizations,\n",
    "    ]),\n",
    "]\n",
    "\n",
    "len(ttas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = MulticlassEfficientNet(\"efficientnet-b0\", 4)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"../logs/efficientnet-b0-c4-continue2/checkpoints/best.pth\",\n",
    "#         \"../logs/efficientnet-b0-without-normalization/checkpoints/best.pth\",\n",
    "        map_location=\"cpu\"\n",
    "    )[\"model_state_dict\"]\n",
    ")\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,\n",
       " ['../data/resized_data/Test/1787.jpg', '../data/resized_data/Test/0312.jpg'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = glob.glob(\"../data/resized_data/Test/*.jpg\")\n",
    "\n",
    "len(test_images), test_images[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA index - 1: 100%|██████████| 79/79 [00:38<00:00,  2.07it/s]\n",
      "TTA index - 2: 100%|██████████| 79/79 [00:37<00:00,  2.08it/s]\n",
      "TTA index - 3: 100%|██████████| 79/79 [00:38<00:00,  2.08it/s]\n",
      "TTA index - 4: 100%|██████████| 79/79 [00:38<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "tta_preds = []\n",
    "for transform_idx, transform in enumerate(ttas, 1):\n",
    "\n",
    "    dataset = ImagesDataset(test_images, transforms=transform)\n",
    "    loader = DataLoader(dataset, batch_size=64, num_workers=16)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=f\"TTA index - {transform_idx}\"):\n",
    "            batch = batch.to(device)\n",
    "            out = 1 - F.softmax(model(batch), dim=1).data.cpu().numpy()[:,0]\n",
    "#             out = torch.sigmoid().detach().cpu().numpy().flatten()\n",
    "            preds.append(out)\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    tta_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000,),\n",
       " array([0.5663949 , 0.97354424, 0.48302984, ..., 0.6806608 , 0.958826  ,\n",
       "        0.66003793], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pred = np.mean(np.stack(tta_preds), 0)\n",
    "avg_pred.shape, avg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0001.jpg</td>\n",
       "      <td>0.094490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0002.jpg</td>\n",
       "      <td>0.637682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0003.jpg</td>\n",
       "      <td>0.591151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0004.jpg</td>\n",
       "      <td>0.625014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0005.jpg</td>\n",
       "      <td>0.824272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id     Label\n",
       "0  0001.jpg  0.094490\n",
       "1  0002.jpg  0.637682\n",
       "2  0003.jpg  0.591151\n",
       "3  0004.jpg  0.625014\n",
       "4  0005.jpg  0.824272"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = ps.DataFrame.from_dict({\n",
    "    \"Id\": [file.rsplit(\"/\", 1)[1] for file in test_images],\n",
    "    \"Label\": avg_pred\n",
    "}).sort_values(by=\"Id\").reset_index(drop=True)\n",
    "\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../submissions/c4_en_b0_hv+flip_continued2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('torch': conda)",
   "language": "python",
   "name": "python37464bittorchcondab46d9803850a4193b40c5aded830a323"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
