{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "\n",
    "import albumentations as alb\n",
    "import albumentations.pytorch\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmdr/anaconda3/envs/torch/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/home/dmdr/anaconda3/envs/torch/lib/python3.7/site-packages/nbformat/notebooknode.py:4: DeprecationWarning:\n",
      "\n",
      "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "\n",
      "/home/dmdr/anaconda3/envs/torch/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import MulticlassEfficientNet\n",
    "from src.datasets import ImagesDataset\n",
    "\n",
    "normalizations = alb.Compose([alb.Normalize(), alb.pytorch.ToTensorV2()])\n",
    "# normalizations = alb.Compose([alb.ToFloat(max_value=255), alb.pytorch.ToTensorV2()])\n",
    "# normalizations = alb.Compose([alb.Normalize(max_pixel_value=1), alb.pytorch.ToTensorV2()])  # IT IS WRONG\n",
    "\n",
    "ttas = [\n",
    "    # original\n",
    "    alb.Compose([\n",
    "        alb.Resize(512, 512),\n",
    "        normalizations,\n",
    "    ]),\n",
    "    # horizontal flipped\n",
    "    alb.Compose([\n",
    "        alb.Resize(512, 512),\n",
    "        alb.HorizontalFlip(p=1),\n",
    "        normalizations,\n",
    "    ]),\n",
    "    # vertical flipped\n",
    "    alb.Compose([\n",
    "        alb.Resize(512, 512),\n",
    "        alb.VerticalFlip(p=1),\n",
    "        normalizations,\n",
    "    ]),\n",
    "    # horizontal & vertical flipped\n",
    "    alb.Compose([\n",
    "        alb.Resize(512, 512),\n",
    "        alb.HorizontalFlip(p=1),\n",
    "        alb.VerticalFlip(p=1),\n",
    "        normalizations,\n",
    "    ]),\n",
    "]\n",
    "\n",
    "len(ttas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_checkpoints(inputs: list):\n",
    "    \"\"\"Loads checkpoints from inputs and returns a model with averaged weights.\n",
    "    \n",
    "    Args:\n",
    "        inputs (List[str]): An iterable of string paths of checkpoints to load from.\n",
    "    \n",
    "    Returns:\n",
    "        A dict of string keys mapping to various values. The 'model' key\n",
    "        from the returned dict should correspond to an OrderedDict mapping\n",
    "        string parameter names to torch Tensors.\n",
    "    \"\"\"\n",
    "    params_dict = collections.OrderedDict()\n",
    "    params_keys = None\n",
    "    new_state = None\n",
    "    num_models = len(inputs)\n",
    "\n",
    "    for f in inputs:\n",
    "        state = torch.load(\n",
    "            f,\n",
    "            map_location=(\n",
    "                lambda s, _: torch.serialization.default_restore_location(s, 'cpu')\n",
    "            ),\n",
    "        )\n",
    "        # Copies over the settings from the first checkpoint\n",
    "        if new_state is None:\n",
    "            new_state = state\n",
    "\n",
    "        model_params = state['model_state_dict']\n",
    "\n",
    "        model_params_keys = list(model_params.keys())\n",
    "        if params_keys is None:\n",
    "            params_keys = model_params_keys\n",
    "        elif params_keys != model_params_keys:\n",
    "            raise KeyError(\n",
    "                'For checkpoint {}, expected list of params: {}, '\n",
    "                'but found: {}'.format(f, params_keys, model_params_keys)\n",
    "            )\n",
    "\n",
    "        for k in params_keys:\n",
    "            p = model_params[k]\n",
    "            if isinstance(p, torch.HalfTensor):\n",
    "                p = p.float()\n",
    "            if k not in params_dict:\n",
    "                params_dict[k] = p.clone()\n",
    "                # NOTE: clone() is needed in case of p is a shared parameter\n",
    "            else:\n",
    "                params_dict[k] += p\n",
    "\n",
    "    averaged_params = collections.OrderedDict()\n",
    "    for k, v in params_dict.items():\n",
    "        averaged_params[k] = v\n",
    "        averaged_params[k].div_(num_models)\n",
    "    new_state['model_state_dict'] = averaged_params\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "swa_state = average_checkpoints([\n",
    "#     \"../logs/efficientnet-b3-c4-continue4/checkpoints/stage1.4.pth\",\n",
    "#     \"../logs/efficientnet-b3-c4-continue4/checkpoints/stage1.2.pth\",\n",
    "#     \"../logs/efficientnet-b3-c4-continue4/checkpoints/stage1.3.pth\",\n",
    "#     \"../logs/efficientnet-b3-c4-continue4/checkpoints/best.pth\",\n",
    "#     \"../logs/efficientnet-b3-c4-continue5/checkpoints/best.pth\",\n",
    "    \n",
    "#     \"../logs/efficientnet-b3-c4-grouped4/checkpoints/stage1.10.pth\",\n",
    "#     \"../logs/efficientnet-b3-c4-grouped4/checkpoints/stage1.9.pth\",\n",
    "#     \"../logs/efficientnet-b3-c4-grouped4/checkpoints/stage1.6.pth\",\n",
    "\n",
    "    \"../logs/efficientnet-b3-c4-grouped5/checkpoints/stage1.6.pth\",\n",
    "    \"../logs/efficientnet-b3-c4-grouped5/checkpoints/stage1.5.pth\",\n",
    "    \"../logs/efficientnet-b3-c4-grouped5/checkpoints/stage1.4.pth\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "model = MulticlassEfficientNet(\"efficientnet-b3\", 4)\n",
    "model.load_state_dict(swa_state[\"model_state_dict\"])\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,\n",
       " ['../data/resized_data/Test/1787.jpg', '../data/resized_data/Test/0312.jpg'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = glob.glob(\"../data/resized_data/Test/*.jpg\")\n",
    "\n",
    "len(test_images), test_images[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA index - 1: 100%|██████████| 79/79 [01:10<00:00,  1.12it/s]\n",
      "TTA index - 2: 100%|██████████| 79/79 [01:10<00:00,  1.12it/s]\n",
      "TTA index - 3: 100%|██████████| 79/79 [01:10<00:00,  1.12it/s]\n",
      "TTA index - 4: 100%|██████████| 79/79 [01:10<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "tta_preds = []\n",
    "for transform_idx, transform in enumerate(ttas, 1):\n",
    "\n",
    "    dataset = ImagesDataset(test_images, transforms=transform)\n",
    "    loader = DataLoader(dataset, batch_size=64, num_workers=16)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=f\"TTA index - {transform_idx}\"):\n",
    "            batch = batch.to(device)\n",
    "            out = 1 - F.softmax(model(batch), dim=1).data.cpu().numpy()[:,0]\n",
    "#             out = torch.sigmoid().detach().cpu().numpy().flatten()\n",
    "            preds.append(out)\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    tta_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000,),\n",
       " array([0.52554893, 0.9713402 , 0.5211836 , ..., 0.6499492 , 0.97256297,\n",
       "        0.61747885], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pred = np.mean(np.stack(tta_preds), 0)\n",
    "avg_pred.shape, avg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0001.jpg</td>\n",
       "      <td>0.052340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0002.jpg</td>\n",
       "      <td>0.625522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0003.jpg</td>\n",
       "      <td>0.516279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0004.jpg</td>\n",
       "      <td>0.638676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0005.jpg</td>\n",
       "      <td>0.965906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id     Label\n",
       "0  0001.jpg  0.052340\n",
       "1  0002.jpg  0.625522\n",
       "2  0003.jpg  0.516279\n",
       "3  0004.jpg  0.638676\n",
       "4  0005.jpg  0.965906"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = ps.DataFrame.from_dict({\n",
    "    \"Id\": [file.rsplit(\"/\", 1)[1] for file in test_images],\n",
    "    \"Label\": avg_pred\n",
    "}).sort_values(by=\"Id\").reset_index(drop=True)\n",
    "\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../submissions/c4_en_b3_hv+flip_swa_group2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('torch': conda)",
   "language": "python",
   "name": "python37464bittorchcondab46d9803850a4193b40c5aded830a323"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
